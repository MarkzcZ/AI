{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 2, 0, 2, 2, 0, 1])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 22\u001b[0m\n\u001b[0;32m     17\u001b[0m b2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.32\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.44\u001b[39m,\u001b[38;5;241m0.70\u001b[39m],requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# l = nn.Sigmoid()\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# s = nn.Softmax(dim=1)\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m a1 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# @ means matrix cross-multiply  1*2 X 2*2\u001b[39;00m\n\u001b[0;32m     23\u001b[0m a2 \u001b[38;5;241m=\u001b[39m a1 \u001b[38;5;241m@\u001b[39m w2 \u001b[38;5;241m+\u001b[39m b2\n\u001b[0;32m     24\u001b[0m yhat \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(a2,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from pyexpat import model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "#set input\n",
    "torch.manual_seed(4321)\n",
    "x = torch.rand(size=(8, 2))\n",
    "y = torch.randint(low=0, high=3, size=(8,))\n",
    "print(y)\n",
    "\n",
    "#init weights & biases\n",
    "w1 = torch.tensor([[0.48,-0.43],[-0.51,-0.48]],requires_grad=True)\n",
    "w2 = torch.tensor([[-0.99,0.36,-0.75],[-0.66,0.34,0.66]],requires_grad=True)\n",
    "b1 = torch.tensor([0.23,0.05],requires_grad=True)\n",
    "b2 = torch.tensor([0.32, -0.44,0.70],requires_grad=True)\n",
    "\n",
    "l = nn.Sigmoid()\n",
    "s = nn.Softmax(dim=1)\n",
    "\n",
    "a1 = l(-(x @ w1 + b1)) # @ means matrix cross-multiply  1*2 X 2*2\n",
    "a2 = a1 @ w2 + b2\n",
    "yhat = s(a2)\n",
    "print(yhat)\n",
    "\n",
    "#predict\n",
    "y_pred = yhat[torch.arange(len(y)),y]\n",
    "print(y_pred)\n",
    "\n",
    "#MSE loss\n",
    "loss = -(torch.log(y_pred)).mean()\n",
    "print(loss)\n",
    "\n",
    "# Calculate the grad\n",
    "loss.backward\n",
    "print(f\"w1's grad = {w1.grad}\")\n",
    "print(f\"w2's grad = {w2.grad}\")\n",
    "print(f\"b1's grad = {b1.grad}\")\n",
    "print(f\"b2's grad = {b2.grad}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6472293f173ec20ef2ce4bf919aa153a4f8d9afcf1b622c25a61ec4a9c799ba4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
