{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'symmetry_mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\M__zzZ\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3799\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\M__zzZ\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\M__zzZ\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'symmetry_mean'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 125\u001b[0m\n\u001b[0;32m    122\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters())\n\u001b[0;32m    124\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, EPOCHS \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m--> 125\u001b[0m     train(model, DEVICE, train_loader, optimizer, epoch)\n\u001b[0;32m    126\u001b[0m     test(model, DEVICE, test_loader)\n",
      "Cell \u001b[1;32mIn [9], line 71\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(model, device, train_loader, optimizer, epoch):\n\u001b[0;32m     70\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> 71\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     72\u001b[0m         data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     73\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\M__zzZ\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    677\u001b[0m class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):\n\u001b[0;32m    678\u001b[0m     r\"\"\"Iterates once over the DataLoader's dataset, as specified by the sampler\"\"\"\n\u001b[0;32m    680\u001b[0m     # NOTE [ Data Loader Multiprocessing Shutdown Logic ]\n\u001b[1;32m--> 681\u001b[0m     #\n\u001b[0;32m    682\u001b[0m     # Preliminary:\n\u001b[0;32m    683\u001b[0m     #\n\u001b[0;32m    684\u001b[0m     # Our data model looks like this (queues are indicated with curly brackets):\n\u001b[0;32m    685\u001b[0m     #\n\u001b[0;32m    686\u001b[0m     #                main process                              ||\n\u001b[0;32m    687\u001b[0m     #                     |                                    ||\n\u001b[0;32m    688\u001b[0m     #               {index_queue}                              ||\n\u001b[0;32m    689\u001b[0m     #                     |                                    ||\n\u001b[0;32m    690\u001b[0m     #              worker processes                            ||     DATA\n\u001b[0;32m    691\u001b[0m     #                     |                                    ||\n\u001b[0;32m    692\u001b[0m     #            {worker_result_queue}                         ||     FLOW\n\u001b[0;32m    693\u001b[0m     #                     |                                    ||\n\u001b[0;32m    694\u001b[0m     #      pin_memory_thread of main process                   ||   DIRECTION\n\u001b[0;32m    695\u001b[0m     #                     |                                    ||\n\u001b[0;32m    696\u001b[0m     #               {data_queue}                               ||\n\u001b[0;32m    697\u001b[0m     #                     |                                    ||\n\u001b[0;32m    698\u001b[0m     #                data output                               \\/\n\u001b[0;32m    699\u001b[0m     #\n\u001b[0;32m    700\u001b[0m     # P.S. `worker_result_queue` and `pin_memory_thread` part may be omitted if\n\u001b[0;32m    701\u001b[0m     #      `pin_memory=False`.\n\u001b[0;32m    702\u001b[0m     #\n\u001b[0;32m    703\u001b[0m     #\n\u001b[0;32m    704\u001b[0m     # Terminating multiprocessing logic requires very careful design. In\n\u001b[0;32m    705\u001b[0m     # particular, we need to make sure that\n\u001b[0;32m    706\u001b[0m     #\n\u001b[0;32m    707\u001b[0m     #   1. The iterator gracefully exits the workers when its last reference is\n\u001b[0;32m    708\u001b[0m     #      gone or it is depleted.\n\u001b[0;32m    709\u001b[0m     #\n\u001b[0;32m    710\u001b[0m     #      In this case, the workers should be gracefully exited because the\n\u001b[0;32m    711\u001b[0m     #      main process may still need to continue to run, and we want cleaning\n\u001b[0;32m    712\u001b[0m     #      up code in the workers to be executed (e.g., releasing GPU memory).\n\u001b[0;32m    713\u001b[0m     #      Naturally, we implement the shutdown logic in `__del__` of\n\u001b[0;32m    714\u001b[0m     #      DataLoaderIterator.\n\u001b[0;32m    715\u001b[0m     #\n\u001b[0;32m    716\u001b[0m     #      We delay the discussion on the logic in this case until later.\n\u001b[0;32m    717\u001b[0m     #\n\u001b[0;32m    718\u001b[0m     #   2. The iterator exits the workers when the loader process and/or worker\n\u001b[0;32m    719\u001b[0m     #      processes exits normally or with error.\n\u001b[0;32m    720\u001b[0m     #\n\u001b[0;32m    721\u001b[0m     #      We set all workers and `pin_memory_thread` to have `daemon=True`.\n\u001b[0;32m    722\u001b[0m     #\n\u001b[0;32m    723\u001b[0m     #      You may ask, why can't we make the workers non-daemonic, and\n\u001b[0;32m    724\u001b[0m     #      gracefully exit using the same logic as we have in `__del__` when the\n\u001b[0;32m    725\u001b[0m     #      iterator gets deleted (see 1 above)?\n\u001b[0;32m    726\u001b[0m     #\n\u001b[0;32m    727\u001b[0m     #      First of all, `__del__` is **not** guaranteed to be called when\n\u001b[0;32m    728\u001b[0m     #      interpreter exits. Even if it is called, by the time it executes,\n\u001b[0;32m    729\u001b[0m     #      many Python core library resources may alreay be freed, and even\n\u001b[0;32m    730\u001b[0m     #      simple things like acquiring an internal lock of a queue may hang.\n\u001b[0;32m    731\u001b[0m     #      Therefore, in this case, we actually need to prevent `__del__` from\n\u001b[0;32m    732\u001b[0m     #      being executed, and rely on the automatic termination of daemonic\n\u001b[0;32m    733\u001b[0m     #      children.\n\u001b[0;32m    734\u001b[0m     #\n\u001b[0;32m    735\u001b[0m     #      Thus, we register an `atexit` hook that sets a global flag\n\u001b[0;32m    736\u001b[0m     #      `_utils.python_exit_status`. Since `atexit` hooks are executed in the\n\u001b[0;32m    737\u001b[0m     #      reverse order of registration, we are guaranteed that this flag is\n\u001b[0;32m    738\u001b[0m     #      set before library resources we use are freed (which, at least in\n\u001b[0;32m    739\u001b[0m     #      CPython, is done via an `atexit` handler defined in\n\u001b[0;32m    740\u001b[0m     #      `multiprocessing/util.py`\n\u001b[0;32m    741\u001b[0m     #      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/util.py#L320-L362\n\u001b[0;32m    742\u001b[0m     #      registered when an object requiring this mechanism is first\n\u001b[0;32m    743\u001b[0m     #      created, e.g., `mp.Queue`\n\u001b[0;32m    744\u001b[0m     #      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/context.py#L100-L103\n\u001b[0;32m    745\u001b[0m     #      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/queues.py#L29\n\u001b[0;32m    746\u001b[0m     #      )\n\u001b[0;32m    747\u001b[0m     #\n\u001b[0;32m    748\u001b[0m     #      So in `__del__`, we check if `_utils.python_exit_status` is set or\n\u001b[0;32m    749\u001b[0m     #      `None` (freed), and perform no-op if so.\n\u001b[0;32m    750\u001b[0m     #\n\u001b[0;32m    751\u001b[0m     #      However, simply letting library clean-up codes run can also be bad,\n\u001b[0;32m    752\u001b[0m     #      because such codes (i.e., `multiprocessing.util._exit_function()`)\n\u001b[0;32m    753\u001b[0m     #      include join putting threads for `mp.Queue`, which can be blocking.\n\u001b[0;32m    754\u001b[0m     #      Hence, the main process putting threads are called with\n\u001b[0;32m    755\u001b[0m     #      `cancel_join_thread` at creation.  See later section\n\u001b[0;32m    756\u001b[0m     #      [ 3b. A process won't hang when putting into a queue; ]\n\u001b[0;32m    757\u001b[0m     #      for more details.\n\u001b[0;32m    758\u001b[0m     #\n\u001b[0;32m    759\u001b[0m     #      Here are two example cases where library clean-up codes can run\n\u001b[0;32m    760\u001b[0m     #      before `__del__` is called:\n\u001b[0;32m    761\u001b[0m     #\n\u001b[0;32m    762\u001b[0m     #        1. If we hold onto a reference to the iterator, it more often\n\u001b[0;32m    763\u001b[0m     #           than not tries to do `multiprocessing` library cleaning before\n\u001b[0;32m    764\u001b[0m     #           clearing the alive referenced objects (https://github.com/pytorch/pytorch/issues/48666)\n\u001b[0;32m    765\u001b[0m     #           and thus prevents our cleaning-up code to run first.\n\u001b[0;32m    766\u001b[0m     #\n\u001b[0;32m    767\u001b[0m     #        2. A similar issue araises when a `DataLoader` is used in a subprocess.\n\u001b[0;32m    768\u001b[0m     #           When a process ends, it shuts the all its daemonic children\n\u001b[0;32m    769\u001b[0m     #           down with a SIGTERM (instead of joining them without a timeout).\n\u001b[0;32m    770\u001b[0m     #           Simiarly for threads, but by a different mechanism. This fact,\n\u001b[0;32m    771\u001b[0m     #           together with a few implementation details of multiprocessing, forces\n\u001b[0;32m    772\u001b[0m     #           us to make workers daemonic. All of our problems arise when a\n\u001b[0;32m    773\u001b[0m     #           DataLoader is used in a subprocess, and are caused by multiprocessing\n\u001b[0;32m    774\u001b[0m     #           code which looks more or less like this:\n\u001b[0;32m    775\u001b[0m     #\n\u001b[0;32m    776\u001b[0m     #               try:\n\u001b[0;32m    777\u001b[0m     #                   your_function_using_a_dataloader()\n\u001b[0;32m    778\u001b[0m     #               finally:\n\u001b[0;32m    779\u001b[0m     #                   multiprocessing.util._exit_function()\n\u001b[0;32m    780\u001b[0m     #\n\u001b[0;32m    781\u001b[0m     #           The joining/termination mentioned above happens inside\n\u001b[0;32m    782\u001b[0m     #           `_exit_function()`. Now, if `your_function_using_a_dataloader()`\n\u001b[0;32m    783\u001b[0m     #           throws, the stack trace stored in the exception will prevent the\n\u001b[0;32m    784\u001b[0m     #           frame which uses `DataLoaderIter` to be freed. If the frame has any\n\u001b[0;32m    785\u001b[0m     #           reference to the `DataLoaderIter` (e.g., in a method of the iter),\n\u001b[0;32m    786\u001b[0m     #           its  `__del__`, which starts the shutdown procedure, will not be\n\u001b[0;32m    787\u001b[0m     #           called. That, in turn, means that workers aren't notified. Attempting\n\u001b[0;32m    788\u001b[0m     #           to join in `_exit_function` will then result in a hang.\n\u001b[0;32m    789\u001b[0m     #\n\u001b[0;32m    790\u001b[0m     #           For context, `_exit_function` is also registered as an `atexit` call.\n\u001b[0;32m    791\u001b[0m     #           So it is unclear to me (@ssnl) why this is needed in a finally block.\n\u001b[0;32m    792\u001b[0m     #           The code dates back to 2008 and there is no comment on the original\n\u001b[0;32m    793\u001b[0m     #           PEP 371 or patch https://bugs.python.org/issue3050 (containing both\n\u001b[0;32m    794\u001b[0m     #           the finally block and the `atexit` registration) that explains this.\n\u001b[0;32m    795\u001b[0m     #\n\u001b[0;32m    796\u001b[0m     #\n\u001b[0;32m    797\u001b[0m     #      Finally, another choice is to just shutdown workers with logic in 1\n\u001b[0;32m    798\u001b[0m     #      above whenever we see an error in `next`. This isn't ideal because\n\u001b[0;32m    799\u001b[0m     #        a. It prevents users from using try-catch to resume data loading.\n\u001b[0;32m    800\u001b[0m     #        b. It doesn't prevent hanging if users have references to the\n\u001b[0;32m    801\u001b[0m     #           iterator.\n\u001b[0;32m    802\u001b[0m     #\n\u001b[0;32m    803\u001b[0m     #   3. All processes exit if any of them die unexpectedly by fatal signals.\n\u001b[0;32m    804\u001b[0m     #\n\u001b[0;32m    805\u001b[0m     #      As shown above, the workers are set as daemonic children of the main\n\u001b[0;32m    806\u001b[0m     #      process. However, automatic cleaning-up of such child processes only\n\u001b[0;32m    807\u001b[0m     #      happens if the parent process exits gracefully (e.g., not via fatal\n\u001b[0;32m    808\u001b[0m     #      signals like SIGKILL). So we must ensure that each process will exit\n\u001b[0;32m    809\u001b[0m     #      even the process that should send/receive data to/from it were\n\u001b[0;32m    810\u001b[0m     #      killed, i.e.,\n\u001b[0;32m    811\u001b[0m     #\n\u001b[0;32m    812\u001b[0m     #        a. A process won't hang when getting from a queue.\n\u001b[0;32m    813\u001b[0m     #\n\u001b[0;32m    814\u001b[0m     #           Even with carefully designed data dependencies (i.e., a `put()`\n\u001b[0;32m    815\u001b[0m     #           always corresponding to a `get()`), hanging on `get()` can still\n\u001b[0;32m    816\u001b[0m     #           happen when data in queue is corrupted (e.g., due to\n\u001b[0;32m    817\u001b[0m     #           `cancel_join_thread` or unexpected exit).\n\u001b[0;32m    818\u001b[0m     #\n\u001b[0;32m    819\u001b[0m     #           For child exit, we set a timeout whenever we try to get data\n\u001b[0;32m    820\u001b[0m     #           from `data_queue`, and check the workers' status on each timeout\n\u001b[0;32m    821\u001b[0m     #           and error.\n\u001b[0;32m    822\u001b[0m     #           See `_DataLoaderiter._get_batch()` and\n\u001b[0;32m    823\u001b[0m     #           `_DataLoaderiter._try_get_data()` for details.\n\u001b[0;32m    824\u001b[0m     #\n\u001b[0;32m    825\u001b[0m     #           Additionally, for child exit on non-Windows platforms, we also\n\u001b[0;32m    826\u001b[0m     #           register a SIGCHLD handler (which is supported on Windows) on\n\u001b[0;32m    827\u001b[0m     #           the main process, which checks if any of the workers fail in the\n\u001b[0;32m    828\u001b[0m     #           (Python) handler. This is more efficient and faster in detecting\n\u001b[0;32m    829\u001b[0m     #           worker failures, compared to only using the above mechanism.\n\u001b[0;32m    830\u001b[0m     #           See `DataLoader.cpp` and `_utils/signal_handling.py` for details.\n\u001b[0;32m    831\u001b[0m     #\n\u001b[0;32m    832\u001b[0m     #           For `.get()` calls where the sender(s) is not the workers, we\n\u001b[0;32m    833\u001b[0m     #           guard them with timeouts, and check the status of the sender\n\u001b[0;32m    834\u001b[0m     #           when timeout happens:\n\u001b[0;32m    835\u001b[0m     #             + in the workers, the `_utils.worker.ManagerWatchdog` class\n\u001b[0;32m    836\u001b[0m     #               checks the status of the main process.\n\u001b[0;32m    837\u001b[0m     #             + if `pin_memory=True`, when getting from `pin_memory_thread`,\n\u001b[0;32m    838\u001b[0m     #               check `pin_memory_thread` status periodically until `.get()`\n\u001b[0;32m    839\u001b[0m     #               returns or see that `pin_memory_thread` died.\n\u001b[0;32m    840\u001b[0m     #\n\u001b[0;32m    841\u001b[0m     #        b. A process won't hang when putting into a queue;\n\u001b[0;32m    842\u001b[0m     #\n\u001b[0;32m    843\u001b[0m     #           We use `mp.Queue` which has a separate background thread to put\n\u001b[0;32m    844\u001b[0m     #           objects from an unbounded buffer array. The background thread is\n\u001b[0;32m    845\u001b[0m     #           daemonic and usually automatically joined when the process\n\u001b[0;32m    846\u001b[0m     #           *exits*.\n\u001b[0;32m    847\u001b[0m     #\n\u001b[0;32m    848\u001b[0m     #           In case that the receiver has ended abruptly while\n\u001b[0;32m    849\u001b[0m     #           reading from the pipe, the join will hang forever.  The usual\n\u001b[0;32m    850\u001b[0m     #           solution for this in Python is calling  `q.cancel_join_thread`,\n\u001b[0;32m    851\u001b[0m     #           which prevents automatically joining it when finalizing\n\u001b[0;32m    852\u001b[0m     #           (exiting).\n\u001b[0;32m    853\u001b[0m     #\n\u001b[0;32m    854\u001b[0m     #           Nonetheless, `cancel_join_thread` must only be called when the\n\u001b[0;32m    855\u001b[0m     #           queue is **not** going to be read from or write into by another\n\u001b[0;32m    856\u001b[0m     #           process, because it may hold onto a lock or leave corrupted data\n\u001b[0;32m    857\u001b[0m     #           in the queue, leading other readers/writers to hang.\n\u001b[0;32m    858\u001b[0m     #\n\u001b[0;32m    859\u001b[0m     #           Hence,\n\u001b[0;32m    860\u001b[0m     #             + For worker processes, we only do so (for their output\n\u001b[0;32m    861\u001b[0m     #               queues, i.e., `worker_result_queue`) before exiting.\n\u001b[0;32m    862\u001b[0m     #             + For `pin_memory_thread`, its output queue `data_queue` is a\n\u001b[0;32m    863\u001b[0m     #               `queue.Queue` that does blocking `put` if the queue is full.\n\u001b[0;32m    864\u001b[0m     #               So there is no above problem, but as a result, in\n\u001b[0;32m    865\u001b[0m     #               `_pin_memory_loop`, we do need to  wrap the `put` in a loop\n\u001b[0;32m    866\u001b[0m     #               that breaks not only upon success, but also when the main\n\u001b[0;32m    867\u001b[0m     #               process stops reading, i.e., is shutting down.\n\u001b[0;32m    868\u001b[0m     #             + For loader process, we `cancel_join_thread()` for all\n\u001b[0;32m    869\u001b[0m     #               `_index_queues` because the whole purpose of workers and\n\u001b[0;32m    870\u001b[0m     #               `pin_memory_thread` is to serve the loader process.  If\n\u001b[0;32m    871\u001b[0m     #               loader process is already exiting, we don't really care if\n\u001b[0;32m    872\u001b[0m     #               the queues are corrupted.\n\u001b[0;32m    873\u001b[0m     #\n\u001b[0;32m    874\u001b[0m     #\n\u001b[0;32m    875\u001b[0m     # Now let's get back to 1:\n\u001b[0;32m    876\u001b[0m     #   how we gracefully exit the workers when the last reference to the\n\u001b[0;32m    877\u001b[0m     #   iterator is gone.\n\u001b[0;32m    878\u001b[0m     #\n\u001b[0;32m    879\u001b[0m     # To achieve this, we implement the following logic along with the design\n\u001b[0;32m    880\u001b[0m     # choices mentioned above:\n\u001b[0;32m    881\u001b[0m     #\n\u001b[0;32m    882\u001b[0m     # `workers_done_event`:\n\u001b[0;32m    883\u001b[0m     #   A `multiprocessing.Event` shared among the main process and all worker\n\u001b[0;32m    884\u001b[0m     #   processes. This is used to signal the workers that the iterator is\n\u001b[0;32m    885\u001b[0m     #   shutting down. After it is set, they will not send processed data to\n\u001b[0;32m    886\u001b[0m     #   queues anymore, and only wait for the final `None` before exiting.\n\u001b[0;32m    887\u001b[0m     #   `done_event` isn't strictly needed. I.e., we can just check for `None`\n\u001b[0;32m    888\u001b[0m     #   from the input queue, but it allows us to skip wasting resources\n\u001b[0;32m    889\u001b[0m     #   processing data if we are already shutting down.\n\u001b[0;32m    890\u001b[0m     #\n\u001b[0;32m    891\u001b[0m     # `pin_memory_thread_done_event`:\n\u001b[0;32m    892\u001b[0m     #   A `threading.Event` for a similar purpose to that of\n\u001b[0;32m    893\u001b[0m     #   `workers_done_event`, but is for the `pin_memory_thread`. The reason\n\u001b[0;32m    894\u001b[0m     #   that separate events are needed is that `pin_memory_thread` reads from\n\u001b[0;32m    895\u001b[0m     #   the output queue of the workers. But the workers, upon seeing that\n\u001b[0;32m    896\u001b[0m     #   `workers_done_event` is set, only wants to see the final `None`, and is\n\u001b[0;32m    897\u001b[0m     #   not required to flush all data in the output queue (e.g., it may call\n\u001b[0;32m    898\u001b[0m     #   `cancel_join_thread` on that queue if its `IterableDataset` iterator\n\u001b[0;32m    899\u001b[0m     #   happens to exhaust coincidentally, which is out of the control of the\n\u001b[0;32m    900\u001b[0m     #   main process). Thus, since we will exit `pin_memory_thread` before the\n\u001b[0;32m    901\u001b[0m     #   workers (see below), two separete events are used.\n\u001b[0;32m    902\u001b[0m     #\n\u001b[0;32m    903\u001b[0m     # NOTE: In short, the protocol is that the main process will set these\n\u001b[0;32m    904\u001b[0m     #       `done_event`s and then the corresponding processes/threads a `None`,\n\u001b[0;32m    905\u001b[0m     #       and that they may exit at any time after receiving the `None`.\n\u001b[0;32m    906\u001b[0m     #\n\u001b[0;32m    907\u001b[0m     # NOTE: Using `None` as the final signal is valid, since normal data will\n\u001b[0;32m    908\u001b[0m     #       always be a 2-tuple with the 1st element being the index of the data\n\u001b[0;32m    909\u001b[0m     #       transferred (different from dataset index/key), and the 2nd being\n\u001b[0;32m    910\u001b[0m     #       either the dataset key or the data sample (depending on which part\n\u001b[0;32m    911\u001b[0m     #       of the data model the queue is at).\n\u001b[0;32m    912\u001b[0m     #\n\u001b[0;32m    913\u001b[0m     # [ worker processes ]\n\u001b[0;32m    914\u001b[0m     #   While loader process is alive:\n\u001b[0;32m    915\u001b[0m     #     Get from `index_queue`.\n\u001b[0;32m    916\u001b[0m     #       If get anything else,\n\u001b[0;32m    917\u001b[0m     #          Check `workers_done_event`.\n\u001b[0;32m    918\u001b[0m     #            If set, continue to next iteration\n\u001b[0;32m    919\u001b[0m     #                    i.e., keep getting until see the `None`, then exit.\n\u001b[0;32m    920\u001b[0m     #            Otherwise, process data:\n\u001b[0;32m    921\u001b[0m     #                If is fetching from an `IterableDataset` and the iterator\n\u001b[0;32m    922\u001b[0m     #                    is exhausted, send an `_IterableDatasetStopIteration`\n\u001b[0;32m    923\u001b[0m     #                    object to signal iteration end. The main process, upon\n\u001b[0;32m    924\u001b[0m     #                    receiving such an object, will send `None` to this\n\u001b[0;32m    925\u001b[0m     #                    worker and not use the corresponding `index_queue`\n\u001b[0;32m    926\u001b[0m     #                    anymore.\n\u001b[0;32m    927\u001b[0m     #       If timed out,\n\u001b[0;32m    928\u001b[0m     #          No matter `workers_done_event` is set (still need to see `None`)\n\u001b[0;32m    929\u001b[0m     #          or not, must continue to next iteration.\n\u001b[0;32m    930\u001b[0m     #   (outside loop)\n\u001b[0;32m    931\u001b[0m     #   If `workers_done_event` is set,  (this can be False with `IterableDataset`)\n\u001b[0;32m    932\u001b[0m     #     `data_queue.cancel_join_thread()`.  (Everything is ending here:\n\u001b[0;32m    933\u001b[0m     #                                          main process won't read from it;\n\u001b[0;32m    934\u001b[0m     #                                          other workers will also call\n\u001b[0;32m    935\u001b[0m     #                                          `cancel_join_thread`.)\n\u001b[0;32m    936\u001b[0m     #\n\u001b[0;32m    937\u001b[0m     # [ pin_memory_thread ]\n\u001b[0;32m    938\u001b[0m     #   # No need to check main thread. If this thread is alive, the main loader\n\u001b[0;32m    939\u001b[0m     #   # thread must be alive, because this thread is set as daemonic.\n\u001b[0;32m    940\u001b[0m     #   While `pin_memory_thread_done_event` is not set:\n\u001b[0;32m    941\u001b[0m     #     Get from `index_queue`.\n\u001b[0;32m    942\u001b[0m     #       If timed out, continue to get in the next iteration.\n\u001b[0;32m    943\u001b[0m     #       Otherwise, process data.\n\u001b[0;32m    944\u001b[0m     #       While `pin_memory_thread_done_event` is not set:\n\u001b[0;32m    945\u001b[0m     #         Put processed data to `data_queue` (a `queue.Queue` with blocking put)\n\u001b[0;32m    946\u001b[0m     #         If timed out, continue to put in the next iteration.\n\u001b[0;32m    947\u001b[0m     #         Otherwise, break, i.e., continuing to the out loop.\n\u001b[0;32m    948\u001b[0m     #\n\u001b[0;32m    949\u001b[0m     #   NOTE: we don't check the status of the main thread because\n\u001b[0;32m    950\u001b[0m     #           1. if the process is killed by fatal signal, `pin_memory_thread`\n\u001b[0;32m    951\u001b[0m     #              ends.\n\u001b[0;32m    952\u001b[0m     #           2. in other cases, either the cleaning-up in __del__ or the\n\u001b[0;32m    953\u001b[0m     #              automatic exit of daemonic thread will take care of it.\n\u001b[0;32m    954\u001b[0m     #              This won't busy-wait either because `.get(timeout)` does not\n\u001b[0;32m    955\u001b[0m     #              busy-wait.\n\u001b[0;32m    956\u001b[0m     #\n\u001b[0;32m    957\u001b[0m     # [ main process ]\n\u001b[0;32m    958\u001b[0m     #   In the DataLoader Iter's `__del__`\n\u001b[0;32m    959\u001b[0m     #     b. Exit `pin_memory_thread`\n\u001b[0;32m    960\u001b[0m     #          i.   Set `pin_memory_thread_done_event`.\n\u001b[0;32m    961\u001b[0m     #          ii   Put `None` in `worker_result_queue`.\n\u001b[0;32m    962\u001b[0m     #          iii. Join the `pin_memory_thread`.\n\u001b[0;32m    963\u001b[0m     #          iv.  `worker_result_queue.cancel_join_thread()`.\n\u001b[0;32m    964\u001b[0m     #\n\u001b[0;32m    965\u001b[0m     #     c. Exit the workers.\n\u001b[0;32m    966\u001b[0m     #          i.   Set `workers_done_event`.\n\u001b[0;32m    967\u001b[0m     #          ii.  Put `None` in each worker's `index_queue`.\n\u001b[0;32m    968\u001b[0m     #          iii. Join the workers.\n\u001b[0;32m    969\u001b[0m     #          iv.  Call `.cancel_join_thread()` on each worker's `index_queue`.\n\u001b[0;32m    970\u001b[0m     #\n\u001b[0;32m    971\u001b[0m     #        NOTE: (c) is better placed after (b) because it may leave corrupted\n\u001b[0;32m    972\u001b[0m     #              data in `worker_result_queue`, which `pin_memory_thread`\n\u001b[0;32m    973\u001b[0m     #              reads from, in which case the `pin_memory_thread` can only\n\u001b[0;32m    974\u001b[0m     #              happen at timeing out, which is slow. Nonetheless, same thing\n\u001b[0;32m    975\u001b[0m     #              happens if a worker is killed by signal at unfortunate times,\n\u001b[0;32m    976\u001b[0m     #              but in other cases, we are better off having a non-corrupted\n\u001b[0;32m    977\u001b[0m     #              `worker_result_queue` for `pin_memory_thread`.\n\u001b[0;32m    978\u001b[0m     #\n\u001b[0;32m    979\u001b[0m     #   NOTE: If `pin_memory=False`, there is no `pin_memory_thread` and (b)\n\u001b[0;32m    980\u001b[0m     #         can be omitted\n\u001b[0;32m    981\u001b[0m     #\n\u001b[0;32m    982\u001b[0m     # NB: `done_event`s isn't strictly needed. E.g., we can just check for\n\u001b[0;32m    983\u001b[0m     #     `None` from `index_queue`, but it allows us to skip wasting resources\n\u001b[0;32m    984\u001b[0m     #     processing indices already in `index_queue` if we are already shutting\n\u001b[0;32m    985\u001b[0m     #     down.\n\u001b[0;32m    987\u001b[0m     def __init__(self, loader):\n\u001b[0;32m    988\u001b[0m         super(_MultiProcessingDataLoaderIter, self).__init__(loader)\n",
      "File \u001b[1;32mc:\\Users\\M__zzZ\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    677\u001b[0m class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):\n\u001b[0;32m    678\u001b[0m     r\"\"\"Iterates once over the DataLoader's dataset, as specified by the sampler\"\"\"\n\u001b[0;32m    680\u001b[0m     # NOTE [ Data Loader Multiprocessing Shutdown Logic ]\n\u001b[0;32m    681\u001b[0m     #\n\u001b[0;32m    682\u001b[0m     # Preliminary:\n\u001b[0;32m    683\u001b[0m     #\n\u001b[0;32m    684\u001b[0m     # Our data model looks like this (queues are indicated with curly brackets):\n\u001b[0;32m    685\u001b[0m     #\n\u001b[0;32m    686\u001b[0m     #                main process                              ||\n\u001b[0;32m    687\u001b[0m     #                     |                                    ||\n\u001b[0;32m    688\u001b[0m     #               {index_queue}                              ||\n\u001b[0;32m    689\u001b[0m     #                     |                                    ||\n\u001b[0;32m    690\u001b[0m     #              worker processes                            ||     DATA\n\u001b[0;32m    691\u001b[0m     #                     |                                    ||\n\u001b[0;32m    692\u001b[0m     #            {worker_result_queue}                         ||     FLOW\n\u001b[0;32m    693\u001b[0m     #                     |                                    ||\n\u001b[0;32m    694\u001b[0m     #      pin_memory_thread of main process                   ||   DIRECTION\n\u001b[0;32m    695\u001b[0m     #                     |                                    ||\n\u001b[0;32m    696\u001b[0m     #               {data_queue}                               ||\n\u001b[0;32m    697\u001b[0m     #                     |                                    ||\n\u001b[0;32m    698\u001b[0m     #                data output                               \\/\n\u001b[0;32m    699\u001b[0m     #\n\u001b[0;32m    700\u001b[0m     # P.S. `worker_result_queue` and `pin_memory_thread` part may be omitted if\n\u001b[0;32m    701\u001b[0m     #      `pin_memory=False`.\n\u001b[0;32m    702\u001b[0m     #\n\u001b[0;32m    703\u001b[0m     #\n\u001b[0;32m    704\u001b[0m     # Terminating multiprocessing logic requires very careful design. In\n\u001b[0;32m    705\u001b[0m     # particular, we need to make sure that\n\u001b[0;32m    706\u001b[0m     #\n\u001b[0;32m    707\u001b[0m     #   1. The iterator gracefully exits the workers when its last reference is\n\u001b[0;32m    708\u001b[0m     #      gone or it is depleted.\n\u001b[0;32m    709\u001b[0m     #\n\u001b[0;32m    710\u001b[0m     #      In this case, the workers should be gracefully exited because the\n\u001b[0;32m    711\u001b[0m     #      main process may still need to continue to run, and we want cleaning\n\u001b[0;32m    712\u001b[0m     #      up code in the workers to be executed (e.g., releasing GPU memory).\n\u001b[0;32m    713\u001b[0m     #      Naturally, we implement the shutdown logic in `__del__` of\n\u001b[0;32m    714\u001b[0m     #      DataLoaderIterator.\n\u001b[0;32m    715\u001b[0m     #\n\u001b[0;32m    716\u001b[0m     #      We delay the discussion on the logic in this case until later.\n\u001b[0;32m    717\u001b[0m     #\n\u001b[0;32m    718\u001b[0m     #   2. The iterator exits the workers when the loader process and/or worker\n\u001b[0;32m    719\u001b[0m     #      processes exits normally or with error.\n\u001b[0;32m    720\u001b[0m     #\n\u001b[1;32m--> 721\u001b[0m     #      We set all workers and `pin_memory_thread` to have `daemon=True`.\n\u001b[0;32m    722\u001b[0m     #\n\u001b[0;32m    723\u001b[0m     #      You may ask, why can't we make the workers non-daemonic, and\n\u001b[0;32m    724\u001b[0m     #      gracefully exit using the same logic as we have in `__del__` when the\n\u001b[0;32m    725\u001b[0m     #      iterator gets deleted (see 1 above)?\n\u001b[0;32m    726\u001b[0m     #\n\u001b[0;32m    727\u001b[0m     #      First of all, `__del__` is **not** guaranteed to be called when\n\u001b[0;32m    728\u001b[0m     #      interpreter exits. Even if it is called, by the time it executes,\n\u001b[0;32m    729\u001b[0m     #      many Python core library resources may alreay be freed, and even\n\u001b[0;32m    730\u001b[0m     #      simple things like acquiring an internal lock of a queue may hang.\n\u001b[0;32m    731\u001b[0m     #      Therefore, in this case, we actually need to prevent `__del__` from\n\u001b[0;32m    732\u001b[0m     #      being executed, and rely on the automatic termination of daemonic\n\u001b[0;32m    733\u001b[0m     #      children.\n\u001b[0;32m    734\u001b[0m     #\n\u001b[0;32m    735\u001b[0m     #      Thus, we register an `atexit` hook that sets a global flag\n\u001b[0;32m    736\u001b[0m     #      `_utils.python_exit_status`. Since `atexit` hooks are executed in the\n\u001b[0;32m    737\u001b[0m     #      reverse order of registration, we are guaranteed that this flag is\n\u001b[0;32m    738\u001b[0m     #      set before library resources we use are freed (which, at least in\n\u001b[0;32m    739\u001b[0m     #      CPython, is done via an `atexit` handler defined in\n\u001b[0;32m    740\u001b[0m     #      `multiprocessing/util.py`\n\u001b[0;32m    741\u001b[0m     #      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/util.py#L320-L362\n\u001b[0;32m    742\u001b[0m     #      registered when an object requiring this mechanism is first\n\u001b[0;32m    743\u001b[0m     #      created, e.g., `mp.Queue`\n\u001b[0;32m    744\u001b[0m     #      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/context.py#L100-L103\n\u001b[0;32m    745\u001b[0m     #      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/queues.py#L29\n\u001b[0;32m    746\u001b[0m     #      )\n\u001b[0;32m    747\u001b[0m     #\n\u001b[0;32m    748\u001b[0m     #      So in `__del__`, we check if `_utils.python_exit_status` is set or\n\u001b[0;32m    749\u001b[0m     #      `None` (freed), and perform no-op if so.\n\u001b[0;32m    750\u001b[0m     #\n\u001b[0;32m    751\u001b[0m     #      However, simply letting library clean-up codes run can also be bad,\n\u001b[0;32m    752\u001b[0m     #      because such codes (i.e., `multiprocessing.util._exit_function()`)\n\u001b[0;32m    753\u001b[0m     #      include join putting threads for `mp.Queue`, which can be blocking.\n\u001b[0;32m    754\u001b[0m     #      Hence, the main process putting threads are called with\n\u001b[0;32m    755\u001b[0m     #      `cancel_join_thread` at creation.  See later section\n\u001b[0;32m    756\u001b[0m     #      [ 3b. A process won't hang when putting into a queue; ]\n\u001b[0;32m    757\u001b[0m     #      for more details.\n\u001b[0;32m    758\u001b[0m     #\n\u001b[0;32m    759\u001b[0m     #      Here are two example cases where library clean-up codes can run\n\u001b[0;32m    760\u001b[0m     #      before `__del__` is called:\n\u001b[0;32m    761\u001b[0m     #\n\u001b[0;32m    762\u001b[0m     #        1. If we hold onto a reference to the iterator, it more often\n\u001b[0;32m    763\u001b[0m     #           than not tries to do `multiprocessing` library cleaning before\n\u001b[0;32m    764\u001b[0m     #           clearing the alive referenced objects (https://github.com/pytorch/pytorch/issues/48666)\n\u001b[0;32m    765\u001b[0m     #           and thus prevents our cleaning-up code to run first.\n\u001b[0;32m    766\u001b[0m     #\n\u001b[0;32m    767\u001b[0m     #        2. A similar issue araises when a `DataLoader` is used in a subprocess.\n\u001b[0;32m    768\u001b[0m     #           When a process ends, it shuts the all its daemonic children\n\u001b[0;32m    769\u001b[0m     #           down with a SIGTERM (instead of joining them without a timeout).\n\u001b[0;32m    770\u001b[0m     #           Simiarly for threads, but by a different mechanism. This fact,\n\u001b[0;32m    771\u001b[0m     #           together with a few implementation details of multiprocessing, forces\n\u001b[0;32m    772\u001b[0m     #           us to make workers daemonic. All of our problems arise when a\n\u001b[0;32m    773\u001b[0m     #           DataLoader is used in a subprocess, and are caused by multiprocessing\n\u001b[0;32m    774\u001b[0m     #           code which looks more or less like this:\n\u001b[0;32m    775\u001b[0m     #\n\u001b[0;32m    776\u001b[0m     #               try:\n\u001b[0;32m    777\u001b[0m     #                   your_function_using_a_dataloader()\n\u001b[0;32m    778\u001b[0m     #               finally:\n\u001b[0;32m    779\u001b[0m     #                   multiprocessing.util._exit_function()\n\u001b[0;32m    780\u001b[0m     #\n\u001b[0;32m    781\u001b[0m     #           The joining/termination mentioned above happens inside\n\u001b[0;32m    782\u001b[0m     #           `_exit_function()`. Now, if `your_function_using_a_dataloader()`\n\u001b[0;32m    783\u001b[0m     #           throws, the stack trace stored in the exception will prevent the\n\u001b[0;32m    784\u001b[0m     #           frame which uses `DataLoaderIter` to be freed. If the frame has any\n\u001b[0;32m    785\u001b[0m     #           reference to the `DataLoaderIter` (e.g., in a method of the iter),\n\u001b[0;32m    786\u001b[0m     #           its  `__del__`, which starts the shutdown procedure, will not be\n\u001b[0;32m    787\u001b[0m     #           called. That, in turn, means that workers aren't notified. Attempting\n\u001b[0;32m    788\u001b[0m     #           to join in `_exit_function` will then result in a hang.\n\u001b[0;32m    789\u001b[0m     #\n\u001b[0;32m    790\u001b[0m     #           For context, `_exit_function` is also registered as an `atexit` call.\n\u001b[0;32m    791\u001b[0m     #           So it is unclear to me (@ssnl) why this is needed in a finally block.\n\u001b[0;32m    792\u001b[0m     #           The code dates back to 2008 and there is no comment on the original\n\u001b[0;32m    793\u001b[0m     #           PEP 371 or patch https://bugs.python.org/issue3050 (containing both\n\u001b[0;32m    794\u001b[0m     #           the finally block and the `atexit` registration) that explains this.\n\u001b[0;32m    795\u001b[0m     #\n\u001b[0;32m    796\u001b[0m     #\n\u001b[0;32m    797\u001b[0m     #      Finally, another choice is to just shutdown workers with logic in 1\n\u001b[0;32m    798\u001b[0m     #      above whenever we see an error in `next`. This isn't ideal because\n\u001b[0;32m    799\u001b[0m     #        a. It prevents users from using try-catch to resume data loading.\n\u001b[0;32m    800\u001b[0m     #        b. It doesn't prevent hanging if users have references to the\n\u001b[0;32m    801\u001b[0m     #           iterator.\n\u001b[0;32m    802\u001b[0m     #\n\u001b[0;32m    803\u001b[0m     #   3. All processes exit if any of them die unexpectedly by fatal signals.\n\u001b[0;32m    804\u001b[0m     #\n\u001b[0;32m    805\u001b[0m     #      As shown above, the workers are set as daemonic children of the main\n\u001b[0;32m    806\u001b[0m     #      process. However, automatic cleaning-up of such child processes only\n\u001b[0;32m    807\u001b[0m     #      happens if the parent process exits gracefully (e.g., not via fatal\n\u001b[0;32m    808\u001b[0m     #      signals like SIGKILL). So we must ensure that each process will exit\n\u001b[0;32m    809\u001b[0m     #      even the process that should send/receive data to/from it were\n\u001b[0;32m    810\u001b[0m     #      killed, i.e.,\n\u001b[0;32m    811\u001b[0m     #\n\u001b[0;32m    812\u001b[0m     #        a. A process won't hang when getting from a queue.\n\u001b[0;32m    813\u001b[0m     #\n\u001b[0;32m    814\u001b[0m     #           Even with carefully designed data dependencies (i.e., a `put()`\n\u001b[0;32m    815\u001b[0m     #           always corresponding to a `get()`), hanging on `get()` can still\n\u001b[0;32m    816\u001b[0m     #           happen when data in queue is corrupted (e.g., due to\n\u001b[0;32m    817\u001b[0m     #           `cancel_join_thread` or unexpected exit).\n\u001b[0;32m    818\u001b[0m     #\n\u001b[0;32m    819\u001b[0m     #           For child exit, we set a timeout whenever we try to get data\n\u001b[0;32m    820\u001b[0m     #           from `data_queue`, and check the workers' status on each timeout\n\u001b[0;32m    821\u001b[0m     #           and error.\n\u001b[0;32m    822\u001b[0m     #           See `_DataLoaderiter._get_batch()` and\n\u001b[0;32m    823\u001b[0m     #           `_DataLoaderiter._try_get_data()` for details.\n\u001b[0;32m    824\u001b[0m     #\n\u001b[0;32m    825\u001b[0m     #           Additionally, for child exit on non-Windows platforms, we also\n\u001b[0;32m    826\u001b[0m     #           register a SIGCHLD handler (which is supported on Windows) on\n\u001b[0;32m    827\u001b[0m     #           the main process, which checks if any of the workers fail in the\n\u001b[0;32m    828\u001b[0m     #           (Python) handler. This is more efficient and faster in detecting\n\u001b[0;32m    829\u001b[0m     #           worker failures, compared to only using the above mechanism.\n\u001b[0;32m    830\u001b[0m     #           See `DataLoader.cpp` and `_utils/signal_handling.py` for details.\n\u001b[0;32m    831\u001b[0m     #\n\u001b[0;32m    832\u001b[0m     #           For `.get()` calls where the sender(s) is not the workers, we\n\u001b[0;32m    833\u001b[0m     #           guard them with timeouts, and check the status of the sender\n\u001b[0;32m    834\u001b[0m     #           when timeout happens:\n\u001b[0;32m    835\u001b[0m     #             + in the workers, the `_utils.worker.ManagerWatchdog` class\n\u001b[0;32m    836\u001b[0m     #               checks the status of the main process.\n\u001b[0;32m    837\u001b[0m     #             + if `pin_memory=True`, when getting from `pin_memory_thread`,\n\u001b[0;32m    838\u001b[0m     #               check `pin_memory_thread` status periodically until `.get()`\n\u001b[0;32m    839\u001b[0m     #               returns or see that `pin_memory_thread` died.\n\u001b[0;32m    840\u001b[0m     #\n\u001b[0;32m    841\u001b[0m     #        b. A process won't hang when putting into a queue;\n\u001b[0;32m    842\u001b[0m     #\n\u001b[0;32m    843\u001b[0m     #           We use `mp.Queue` which has a separate background thread to put\n\u001b[0;32m    844\u001b[0m     #           objects from an unbounded buffer array. The background thread is\n\u001b[0;32m    845\u001b[0m     #           daemonic and usually automatically joined when the process\n\u001b[0;32m    846\u001b[0m     #           *exits*.\n\u001b[0;32m    847\u001b[0m     #\n\u001b[0;32m    848\u001b[0m     #           In case that the receiver has ended abruptly while\n\u001b[0;32m    849\u001b[0m     #           reading from the pipe, the join will hang forever.  The usual\n\u001b[0;32m    850\u001b[0m     #           solution for this in Python is calling  `q.cancel_join_thread`,\n\u001b[0;32m    851\u001b[0m     #           which prevents automatically joining it when finalizing\n\u001b[0;32m    852\u001b[0m     #           (exiting).\n\u001b[0;32m    853\u001b[0m     #\n\u001b[0;32m    854\u001b[0m     #           Nonetheless, `cancel_join_thread` must only be called when the\n\u001b[0;32m    855\u001b[0m     #           queue is **not** going to be read from or write into by another\n\u001b[0;32m    856\u001b[0m     #           process, because it may hold onto a lock or leave corrupted data\n\u001b[0;32m    857\u001b[0m     #           in the queue, leading other readers/writers to hang.\n\u001b[0;32m    858\u001b[0m     #\n\u001b[0;32m    859\u001b[0m     #           Hence,\n\u001b[0;32m    860\u001b[0m     #             + For worker processes, we only do so (for their output\n\u001b[0;32m    861\u001b[0m     #               queues, i.e., `worker_result_queue`) before exiting.\n\u001b[0;32m    862\u001b[0m     #             + For `pin_memory_thread`, its output queue `data_queue` is a\n\u001b[0;32m    863\u001b[0m     #               `queue.Queue` that does blocking `put` if the queue is full.\n\u001b[0;32m    864\u001b[0m     #               So there is no above problem, but as a result, in\n\u001b[0;32m    865\u001b[0m     #               `_pin_memory_loop`, we do need to  wrap the `put` in a loop\n\u001b[0;32m    866\u001b[0m     #               that breaks not only upon success, but also when the main\n\u001b[0;32m    867\u001b[0m     #               process stops reading, i.e., is shutting down.\n\u001b[0;32m    868\u001b[0m     #             + For loader process, we `cancel_join_thread()` for all\n\u001b[0;32m    869\u001b[0m     #               `_index_queues` because the whole purpose of workers and\n\u001b[0;32m    870\u001b[0m     #               `pin_memory_thread` is to serve the loader process.  If\n\u001b[0;32m    871\u001b[0m     #               loader process is already exiting, we don't really care if\n\u001b[0;32m    872\u001b[0m     #               the queues are corrupted.\n\u001b[0;32m    873\u001b[0m     #\n\u001b[0;32m    874\u001b[0m     #\n\u001b[0;32m    875\u001b[0m     # Now let's get back to 1:\n\u001b[0;32m    876\u001b[0m     #   how we gracefully exit the workers when the last reference to the\n\u001b[0;32m    877\u001b[0m     #   iterator is gone.\n\u001b[0;32m    878\u001b[0m     #\n\u001b[0;32m    879\u001b[0m     # To achieve this, we implement the following logic along with the design\n\u001b[0;32m    880\u001b[0m     # choices mentioned above:\n\u001b[0;32m    881\u001b[0m     #\n\u001b[0;32m    882\u001b[0m     # `workers_done_event`:\n\u001b[0;32m    883\u001b[0m     #   A `multiprocessing.Event` shared among the main process and all worker\n\u001b[0;32m    884\u001b[0m     #   processes. This is used to signal the workers that the iterator is\n\u001b[0;32m    885\u001b[0m     #   shutting down. After it is set, they will not send processed data to\n\u001b[0;32m    886\u001b[0m     #   queues anymore, and only wait for the final `None` before exiting.\n\u001b[0;32m    887\u001b[0m     #   `done_event` isn't strictly needed. I.e., we can just check for `None`\n\u001b[0;32m    888\u001b[0m     #   from the input queue, but it allows us to skip wasting resources\n\u001b[0;32m    889\u001b[0m     #   processing data if we are already shutting down.\n\u001b[0;32m    890\u001b[0m     #\n\u001b[0;32m    891\u001b[0m     # `pin_memory_thread_done_event`:\n\u001b[0;32m    892\u001b[0m     #   A `threading.Event` for a similar purpose to that of\n\u001b[0;32m    893\u001b[0m     #   `workers_done_event`, but is for the `pin_memory_thread`. The reason\n\u001b[0;32m    894\u001b[0m     #   that separate events are needed is that `pin_memory_thread` reads from\n\u001b[0;32m    895\u001b[0m     #   the output queue of the workers. But the workers, upon seeing that\n\u001b[0;32m    896\u001b[0m     #   `workers_done_event` is set, only wants to see the final `None`, and is\n\u001b[0;32m    897\u001b[0m     #   not required to flush all data in the output queue (e.g., it may call\n\u001b[0;32m    898\u001b[0m     #   `cancel_join_thread` on that queue if its `IterableDataset` iterator\n\u001b[0;32m    899\u001b[0m     #   happens to exhaust coincidentally, which is out of the control of the\n\u001b[0;32m    900\u001b[0m     #   main process). Thus, since we will exit `pin_memory_thread` before the\n\u001b[0;32m    901\u001b[0m     #   workers (see below), two separete events are used.\n\u001b[0;32m    902\u001b[0m     #\n\u001b[0;32m    903\u001b[0m     # NOTE: In short, the protocol is that the main process will set these\n\u001b[0;32m    904\u001b[0m     #       `done_event`s and then the corresponding processes/threads a `None`,\n\u001b[0;32m    905\u001b[0m     #       and that they may exit at any time after receiving the `None`.\n\u001b[0;32m    906\u001b[0m     #\n\u001b[0;32m    907\u001b[0m     # NOTE: Using `None` as the final signal is valid, since normal data will\n\u001b[0;32m    908\u001b[0m     #       always be a 2-tuple with the 1st element being the index of the data\n\u001b[0;32m    909\u001b[0m     #       transferred (different from dataset index/key), and the 2nd being\n\u001b[0;32m    910\u001b[0m     #       either the dataset key or the data sample (depending on which part\n\u001b[0;32m    911\u001b[0m     #       of the data model the queue is at).\n\u001b[0;32m    912\u001b[0m     #\n\u001b[0;32m    913\u001b[0m     # [ worker processes ]\n\u001b[0;32m    914\u001b[0m     #   While loader process is alive:\n\u001b[0;32m    915\u001b[0m     #     Get from `index_queue`.\n\u001b[0;32m    916\u001b[0m     #       If get anything else,\n\u001b[0;32m    917\u001b[0m     #          Check `workers_done_event`.\n\u001b[0;32m    918\u001b[0m     #            If set, continue to next iteration\n\u001b[0;32m    919\u001b[0m     #                    i.e., keep getting until see the `None`, then exit.\n\u001b[0;32m    920\u001b[0m     #            Otherwise, process data:\n\u001b[0;32m    921\u001b[0m     #                If is fetching from an `IterableDataset` and the iterator\n\u001b[0;32m    922\u001b[0m     #                    is exhausted, send an `_IterableDatasetStopIteration`\n\u001b[0;32m    923\u001b[0m     #                    object to signal iteration end. The main process, upon\n\u001b[0;32m    924\u001b[0m     #                    receiving such an object, will send `None` to this\n\u001b[0;32m    925\u001b[0m     #                    worker and not use the corresponding `index_queue`\n\u001b[0;32m    926\u001b[0m     #                    anymore.\n\u001b[0;32m    927\u001b[0m     #       If timed out,\n\u001b[0;32m    928\u001b[0m     #          No matter `workers_done_event` is set (still need to see `None`)\n\u001b[0;32m    929\u001b[0m     #          or not, must continue to next iteration.\n\u001b[0;32m    930\u001b[0m     #   (outside loop)\n\u001b[0;32m    931\u001b[0m     #   If `workers_done_event` is set,  (this can be False with `IterableDataset`)\n\u001b[0;32m    932\u001b[0m     #     `data_queue.cancel_join_thread()`.  (Everything is ending here:\n\u001b[0;32m    933\u001b[0m     #                                          main process won't read from it;\n\u001b[0;32m    934\u001b[0m     #                                          other workers will also call\n\u001b[0;32m    935\u001b[0m     #                                          `cancel_join_thread`.)\n\u001b[0;32m    936\u001b[0m     #\n\u001b[0;32m    937\u001b[0m     # [ pin_memory_thread ]\n\u001b[0;32m    938\u001b[0m     #   # No need to check main thread. If this thread is alive, the main loader\n\u001b[0;32m    939\u001b[0m     #   # thread must be alive, because this thread is set as daemonic.\n\u001b[0;32m    940\u001b[0m     #   While `pin_memory_thread_done_event` is not set:\n\u001b[0;32m    941\u001b[0m     #     Get from `index_queue`.\n\u001b[0;32m    942\u001b[0m     #       If timed out, continue to get in the next iteration.\n\u001b[0;32m    943\u001b[0m     #       Otherwise, process data.\n\u001b[0;32m    944\u001b[0m     #       While `pin_memory_thread_done_event` is not set:\n\u001b[0;32m    945\u001b[0m     #         Put processed data to `data_queue` (a `queue.Queue` with blocking put)\n\u001b[0;32m    946\u001b[0m     #         If timed out, continue to put in the next iteration.\n\u001b[0;32m    947\u001b[0m     #         Otherwise, break, i.e., continuing to the out loop.\n\u001b[0;32m    948\u001b[0m     #\n\u001b[0;32m    949\u001b[0m     #   NOTE: we don't check the status of the main thread because\n\u001b[0;32m    950\u001b[0m     #           1. if the process is killed by fatal signal, `pin_memory_thread`\n\u001b[0;32m    951\u001b[0m     #              ends.\n\u001b[0;32m    952\u001b[0m     #           2. in other cases, either the cleaning-up in __del__ or the\n\u001b[0;32m    953\u001b[0m     #              automatic exit of daemonic thread will take care of it.\n\u001b[0;32m    954\u001b[0m     #              This won't busy-wait either because `.get(timeout)` does not\n\u001b[0;32m    955\u001b[0m     #              busy-wait.\n\u001b[0;32m    956\u001b[0m     #\n\u001b[0;32m    957\u001b[0m     # [ main process ]\n\u001b[0;32m    958\u001b[0m     #   In the DataLoader Iter's `__del__`\n\u001b[0;32m    959\u001b[0m     #     b. Exit `pin_memory_thread`\n\u001b[0;32m    960\u001b[0m     #          i.   Set `pin_memory_thread_done_event`.\n\u001b[0;32m    961\u001b[0m     #          ii   Put `None` in `worker_result_queue`.\n\u001b[0;32m    962\u001b[0m     #          iii. Join the `pin_memory_thread`.\n\u001b[0;32m    963\u001b[0m     #          iv.  `worker_result_queue.cancel_join_thread()`.\n\u001b[0;32m    964\u001b[0m     #\n\u001b[0;32m    965\u001b[0m     #     c. Exit the workers.\n\u001b[0;32m    966\u001b[0m     #          i.   Set `workers_done_event`.\n\u001b[0;32m    967\u001b[0m     #          ii.  Put `None` in each worker's `index_queue`.\n\u001b[0;32m    968\u001b[0m     #          iii. Join the workers.\n\u001b[0;32m    969\u001b[0m     #          iv.  Call `.cancel_join_thread()` on each worker's `index_queue`.\n\u001b[0;32m    970\u001b[0m     #\n\u001b[0;32m    971\u001b[0m     #        NOTE: (c) is better placed after (b) because it may leave corrupted\n\u001b[0;32m    972\u001b[0m     #              data in `worker_result_queue`, which `pin_memory_thread`\n\u001b[0;32m    973\u001b[0m     #              reads from, in which case the `pin_memory_thread` can only\n\u001b[0;32m    974\u001b[0m     #              happen at timeing out, which is slow. Nonetheless, same thing\n\u001b[0;32m    975\u001b[0m     #              happens if a worker is killed by signal at unfortunate times,\n\u001b[0;32m    976\u001b[0m     #              but in other cases, we are better off having a non-corrupted\n\u001b[0;32m    977\u001b[0m     #              `worker_result_queue` for `pin_memory_thread`.\n\u001b[0;32m    978\u001b[0m     #\n\u001b[0;32m    979\u001b[0m     #   NOTE: If `pin_memory=False`, there is no `pin_memory_thread` and (b)\n\u001b[0;32m    980\u001b[0m     #         can be omitted\n\u001b[0;32m    981\u001b[0m     #\n\u001b[0;32m    982\u001b[0m     # NB: `done_event`s isn't strictly needed. E.g., we can just check for\n\u001b[0;32m    983\u001b[0m     #     `None` from `index_queue`, but it allows us to skip wasting resources\n\u001b[0;32m    984\u001b[0m     #     processing indices already in `index_queue` if we are already shutting\n\u001b[0;32m    985\u001b[0m     #     down.\n\u001b[0;32m    987\u001b[0m     def __init__(self, loader):\n\u001b[0;32m    988\u001b[0m         super(_MultiProcessingDataLoaderIter, self).__init__(loader)\n",
      "File \u001b[1;32mc:\\Users\\M__zzZ\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, dataset, auto_collation, collate_fn, drop_last):\n\u001b[1;32m---> 49\u001b[0m     \u001b[39msuper\u001b[39m(_MapDatasetFetcher, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m     50\u001b[0m         dataset, auto_collation, collate_fn, drop_last\n\u001b[0;32m     51\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\M__zzZ\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, dataset, auto_collation, collate_fn, drop_last):\n\u001b[1;32m---> 49\u001b[0m     \u001b[39msuper\u001b[39m(_MapDatasetFetcher, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m     50\u001b[0m         dataset, auto_collation, collate_fn, drop_last\n\u001b[0;32m     51\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\M__zzZ\\Desktop\\assignment\\AI\\final_project\\Dataset.py:15\u001b[0m, in \u001b[0;36mMyDataSet.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m     14\u001b[0m     \u001b[39m# Tensorasv\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     x1_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[\u001b[39m'\u001b[39;49m\u001b[39msymmetry_mean\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39mto_list())\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m     list1 \u001b[39m=\u001b[39m []\n\u001b[0;32m     18\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39mdiagnosis\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\M__zzZ\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\M__zzZ\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'symmetry_mean'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#\n",
    "from Dataset import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Inception(torch.nn.Module):\n",
    "    #  Inception\n",
    "    # ninput\n",
    "    # n111\n",
    "    # n3pren333\n",
    "    # n5pren555\n",
    "    # npool\n",
    "    def __init__(self, ninput, n1, n3pre, n3, n5pre, n5, npool):\n",
    "        super(Inception, self).__init__()\n",
    "        self.b1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(ninput, n1, kernel_size=1),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.b2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(ninput, n3pre, kernel_size=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(n3pre, n3, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.b3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(ninput, n5pre, kernel_size=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(n5pre, n5, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.b4 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outpus = [self.b1(x), self.b2(x), self.b3(x), self.b4(x)]\n",
    "        return torch.cat(outpus, dim=1)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv2d(74, 20, kernel_size=5)\n",
    "\n",
    "        self.incep1 = Inception(10, 16, 16, 24, 16, 24, 24)\n",
    "        self.incep2 = Inception(20, 16, 16, 24, 16, 24, 24)\n",
    "\n",
    "        self.mp = torch.nn.MaxPool2d(2)\n",
    "        self.fc = torch.nn.Linear(1344, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        out = F.relu(self.mp(self.conv1(x)))\n",
    "        out = self.incep1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.mp(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.incep2(out)\n",
    "        out = out.view(in_size, -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % 30 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.4f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.max(1, keepdim=True)[1]  # \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    print('\\nAccuracy: {}/{} ({:.4f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BATCH_SIZE = 512  # 2G\n",
    "    EPOCHS = 10  # \n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    \n",
    "\n",
    "    # \n",
    "    myTrainData = MyDataSet(\"ecg_data.csv\")\n",
    "    # batch_size5050\n",
    "    train_loader = DataLoader(dataset=myTrainData, batch_size=50, shuffle=True)\n",
    "\n",
    "    # \n",
    "    myTestData = MyDataSet(\"ecg_data.csv\")\n",
    "    test_loader = DataLoader(dataset=myTestData, batch_size=50, shuffle=True)\n",
    "\n",
    "    # for i, img_data in enumerate(train_loader, 1):\n",
    "    #     images, labels = img_data\n",
    "    #     print('batch{0}:images shape info-->{1} labels-->{2}'.format(i, images.shape, labels))\n",
    "\n",
    "    model = Net().to(DEVICE)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "        test(model, DEVICE, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6472293f173ec20ef2ce4bf919aa153a4f8d9afcf1b622c25a61ec4a9c799ba4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
